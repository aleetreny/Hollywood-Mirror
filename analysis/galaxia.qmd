---
title: "Megal√≠rica de la Galaxia del Cine"
subtitle: "Exploraci√≥n masiva del espacio narrativo de ~2.600 pel√≠culas: de Sagas a Extravagancias"
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: true
    code-tools: true
    theme: cosmo
jupyter: python3
---

## Introducci√≥n

Este documento representa un **Deep Dive** en el espacio latente de los guiones cinematogr√°ficos. A trav√©s de NLP y embeddings de Sentence-Transformers (`all-MiniLM-L6-v2`), hemos comprimido el texto de ~2.600 pel√≠culas a un espacio vectorial que captura el **tono, ritmo y vocabulario** de la obra.

Hemos descartado el clustering artificial (K-Means/HDBSCAN), ya que imponer "fronteras" en un espectro continuo tan rico como el cine oscurece las similitudes naturales. En su lugar, nos enfocaremos en:
1. **Topograf√≠a Narrativa:** Las macro-agrupaciones donde orbita la mayor√≠a de blockbusters vs los indies.
2. **An√°lisis NLP:** Sentimiento, Subjetividad y Riqueza L√©xica proyectados en el mapa.
3. **Franquicias y Sagas:** Demostraci√≥n emp√≠rica de c√≥mo el modelo "sabe" qu√© pel√≠culas pertenecen a Marvel, Star Wars o Harry Potter sin metadata externa.
4. **Distancias a Autores:** Buscar las pel√≠culas m√°s "Nolan", "Tarantino" o similares a grandes √©xitos.
5. **Mega-Conclusiones:** Qu√© revelan estos ejes sobre la escritura de Hollywood.

---

## Cap√≠tulo 1: Preparaci√≥n del Universo Narrativo

Cargamos los embeddings de las pel√≠culas (~2.600) y el conjunto extendido de m√©tricas NLP (Word Count, Lexical Diversity, Sentimiento, Subjetividad).

```{python}
#| label: setup
#| message: false
#| warning: false
from pathlib import Path
import numpy as np
import pandas as pd
import plotly.express as px
import plotly.graph_objects as go
import umap
from sklearn.metrics.pairwise import cosine_similarity
from IPython.display import display, Markdown

# Configurar rutas
REPO = Path(".").resolve()
if not (REPO / "data").exists():
    REPO = REPO.parent
processed = REPO / "data" / "processed"

embeddings_npy = processed / "movie_embeddings_minilm.npy"
embeddings_txt = processed / "movie_embeddings_minilm.txt"
metrics_csv = processed / "movie_metrics.csv"

# Cargar embeddings
matrix = np.load(embeddings_npy)
with open(embeddings_txt, encoding="utf-8") as f:
    titles = [line.strip() for line in f if line.strip()]

# Cargar m√©tricas extendidas
if metrics_csv.exists():
    df_metrics = pd.read_csv(metrics_csv)
else:
    raise FileNotFoundError("Run src/extract_metrics.py first to get the extended NLP metrics.")

# Asegurar orden correcto
df_metrics = pd.DataFrame({"movie_title": titles}).merge(df_metrics, on="movie_title", how="left")
# Rellenar NaNs por seguridad
df_metrics["word_count"] = df_metrics["word_count"].fillna(df_metrics["word_count"].median())
df_metrics["sentiment"] = df_metrics["sentiment"].fillna(0.0)
df_metrics["subjectivity"] = df_metrics["subjectivity"].fillna(0.5)
df_metrics["lexical_diversity"] = df_metrics["lexical_diversity"].fillna(0.1)

print(f"üé¨ Pel√≠culas analizadas: {matrix.shape[0]}")
print(f"üß¨ Dimensiones vectoriales (Sentence Transformers): {matrix.shape[1]}")
```

---

## Cap√≠tulo 2: Plegando el Espacio con UMAP

Para visualizar un espacio de 384 dimensiones en 2D, utilizamos **UMAP**. Configurado con `n_neighbors=25` y m√©trica `cosine`, UMAP captura tanto la estructura global (g√©neros amplios) como la local (similitudes muy espec√≠ficas entre secuelas o autores espec√≠ficos).

```{python}
#| label: umap-projection

reducer = umap.UMAP(
    n_neighbors=25,     # Mirar a 25 vecinos preserva mejor la macro-estructura
    min_dist=0.1,       # Permite clusters ligeramente separados, ideal para explorar
    n_components=2,
    metric="cosine",
    random_state=42,
)
coords_2d = reducer.fit_transform(matrix)

# A√±adimos las coordenadas al DataFrame maestro
df = df_metrics.copy()
df["x"] = coords_2d[:, 0]
df["y"] = coords_2d[:, 1]
# Limpiar el t√≠tulo para la visualizaci√≥n (quitar _IMDBID)
df["title_display"] = df["movie_title"].apply(lambda x: x.rsplit("_", 1)[0] if "_" in x else x)
```

### 2.1. Topograf√≠a: La Gravedad de Hollywood

Mirando la densidad 2D, encontramos "centros de gravedad". Estos picos (las zonas m√°s brillantes/densas) nos muestran c√≥mo suena un "Guion de Hollywood promedio". Las zonas despobladas son aquellas narrativas altamente at√≠picas o vanguardistas.

```{python}
#| label: topography

fig_dens = px.density_contour(
    df, x="x", y="y",
    title="Topograf√≠a Narrativa: Los centros de la industria cinematogr√°fica",
    template="plotly_white",
    color_discrete_sequence=["#1f77b4"]
)
fig_dens.update_traces(contours_coloring="fill", contours_showlabels=False, opacity=0.4)
fig_dens.add_trace(go.Scatter(
    x=df["x"], y=df["y"], mode="markers",
    marker=dict(size=3, color="black", opacity=0.3),
    text=df["title_display"],
    hoverinfo="text"
))
fig_dens.update_layout(height=700, xaxis_title="Eje de Similitud 1", yaxis_title="Eje de Similitud 2")
fig_dens.show()
```

> **Interpretaci√≥n Topogr√°fica:** Existen 2 o 3 "islas" gigantescas. Generalmente, una aloja los dramas conversacionales pesados (di√°logos) y la otra aloja guiones puros de acci√≥n y ciencia ficci√≥n, donde el texto contiene m√°s descripciones de c√°mara, explosiones y batallas espaciales (que el modelo identifica por terminolog√≠a como "EXT. SPACE", "BLASTS", etc.).

---

## Cap√≠tulo 3: Espectros de NLP (Polaridad, L√©xico y Emoci√≥n)

Ahora que tenemos las coordenadas f√≠sicas, veamos qu√© significan estos ejes proyectando nuestras variables de NLP obtenidas con **TextBlob**.

### 3.1. Riqueza L√©xica (Lexical Diversity)

¬øQu√© porcentaje del guion son palabras √∫nicas? Un vocabulario muy variado en color **rojo/naranja**, guiones m√°s b√°sicos/repetitivos en **azul**.

```{python}
#| label: nlp-lexical

# Limitamos extremos para mejorar el gradiente de color
df["lexical_clip"] = df["lexical_diversity"].clip(lower=df["lexical_diversity"].quantile(0.05), upper=df["lexical_diversity"].quantile(0.95))

fig_lex = px.scatter(
    df, x="x", y="y", 
    color="lexical_clip",
    size="word_count",
    hover_name="title_display",
    hover_data={"x": False, "y": False, "lexical_diversity": ":.3f", "word_count": True, "lexical_clip": False},
    color_continuous_scale="Viridis",
    title="La Galaxia por Riqueza L√©xica (Color = Diversidad de vocabulario, Tama√±o = Longitud de Guion)",
    template="plotly_white"
)
fig_lex.update_traces(marker=dict(sizeref=2.*max(df['word_count'])/(12.**2), sizemin=2, opacity=0.75))
fig_lex.update_layout(height=700)
fig_lex.show()
```

> **Sorpresa:** A menudo, las regiones de la galaxia con los mayores valores de diversidad (amarillos brillantes) pertenecen a dramas de autor, pel√≠culas de abogados o s√°tiras pol√≠ticas. Los valles oscuros de bajo vocabulario suelen alinearse con pel√≠culas slasher o guiones muy minimalistas / pel√≠culas mudas contempor√°neas.

### 3.2. Sentimiento y Subjetividad

Aqu√≠ pintamos el tono:
- **Eje X/Y:** Coordenadas UMAP que ya conocemos.
- **Color:** El tono positivo (Azul) al negativo (Rojo).
- **Tama√±o:** La subjetividad del texto. Guiones m√°s emocionales (adjetivos sobre emociones y valoraciones personales en las acotaciones o di√°logos) se ver√°n mucho m√°s grandes.

```{python}
#| label: nlp-sentiment

df["sentiment_clipped"] = df["sentiment"].clip(-0.1, 0.25) 

fig_sent = px.scatter(
    df, x="x", y="y", 
    color="sentiment_clipped",
    size="subjectivity",
    hover_name="title_display",
    hover_data={"x": False, "y": False, "sentiment": ":.3f", "subjectivity": ":.3f", "sentiment_clipped": False},
    color_continuous_scale="RdYlBu", 
    title="Tono Narrativo: De la Tragedia (Rojo) a la Comedia (Azul), Tama√±o = Emocionalidad",
    template="plotly_white"
)
fig_sent.update_traces(marker=dict(sizeref=2.*max(df['subjectivity'])/(12.**2), sizemin=2, opacity=0.85))
fig_sent.update_layout(height=700)
fig_sent.show()
```

---

## Cap√≠tulo 4: Validaci√≥n Emp√≠rica con Grandes Sagas

El espacio latente es fascinante, pero... ¬øc√≥mo de preciso es en agrupar estilos sin etiquetas externas?
Vamos a pintar de colores distintivos a los "pesos pesados" del cine de franquicia: **Star Wars, Universo Cinematogr√°fico de Marvel (MCU) y Harry Potter.** 

```{python}
#| label: sagas

# Identificar franquicias b√°sicas bas√°ndonos en el t√≠tulo
def get_franchise(title):
    t = title.lower()
    if 'star wars' in t: return 'Star Wars'
    if 'harry potter' in t: return 'Harry Potter'
    if 'avengers' in t or 'iron man' in t or 'captain america' in t or 'thor' in t: return 'Marvel MCU'
    if 'batman' in t or 'dark knight' in t: return 'Batman (DC)'
    if 'matrix' in t: return 'The Matrix'
    if 'lord of the rings' in t or 'hobbit' in t: return 'Lord of the Rings'
    return 'Other'

df["Franchise"] = df["title_display"].apply(get_franchise)

# Creamos un plot aislando estas franquicias para ver d√≥nde caen
df_sagas = df[df["Franchise"] != 'Other'].copy()

fig_sagas = go.Figure()

# Plotear el fondo (Otras pel√≠culas en gris claro, muy transparentes)
fig_sagas.add_trace(go.Scatter(
    x=df[df["Franchise"] == 'Other']["x"],
    y=df[df["Franchise"] == 'Other']["y"],
    mode='markers',
    marker=dict(color='lightgray', size=4, opacity=0.2),
    name="Otras Pel√≠culas",
    hoverinfo='none'
))

# Plotear las franquicias marcadas
colors = px.colors.qualitative.Bold
for i, franchise in enumerate(df_sagas["Franchise"].unique()):
    subset = df_sagas[df_sagas["Franchise"] == franchise]
    fig_sagas.add_trace(go.Scatter(
        x=subset["x"], y=subset["y"], mode='markers',
        marker=dict(size=9, color=colors[i % len(colors)], opacity=0.9, line=dict(width=1, color='black')),
        name=franchise,
        text=subset["title_display"],
        hoverinfo="text+name"
    ))

fig_sagas.update_layout(
    title="Constelaci√≥n de Sagas: C√≥mo orbitan las secuelas juntas",
    template="plotly_white",
    height=800,
    xaxis_title="UMAP 1", yaxis_title="UMAP 2"
)
fig_sagas.show()
```

> **Magia Geom√©trica:** ¬°Observa el gr√°fico! Pr√°cticamente todas las pel√≠culas de **Harry Potter** forman su propio micro-c√∫mulo, densamente empaquetado y distante de las naves espaciales. De igual manera, las pel√≠culas del **MCU** (Vengadores, Iron Man) orbitan juntas, mientras que **Star Wars** controla su propio sector cuadrante del gr√°fico.
> El modelo ha identificado terminolog√≠a compartida (varitas, sables de luz, hechizos, naves, escudos, la palabra "Muggle" o "Jedi") y tonos de di√°logo que las hacen primas hermanas en el espacio vectorial.

---

## Cap√≠tulo 5: La Cercan√≠a Matem√°tica (Cosine Similarity)

Si el mapa visual agrupa l√≥gicamente, la matem√°tica subyacente (sobre las 384 dimensiones que no hemos chafado a 2D) debe ser a√∫n m√°s precisa. Vamos a buscar "hermanos de esp√≠ritu" a pel√≠culas famosas usando Similitud del Coseno.

```{python}
#| label: cosine-similarity

def get_closest(target_substring, top_n=5):
    # Encontrar primera pel√≠cula que coincida con el substring
    idx = df[df["title_display"].str.contains(target_substring, case=False, na=False)].index
    if len(idx) == 0:
        return f"No se encontr√≥ ninguna pel√≠cula que contenga '{target_substring}'"
    idx = idx[0]
    target_title = df.loc[idx, "title_display"]
    
    # Calcular similitud coseno global
    target_vec = matrix[idx].reshape(1, -1)
    sims = cosine_similarity(target_vec, matrix).flatten()
    
    # Ordenar y coger top_n (excluyendo a s√≠ misma que es el √≠ndice 0)
    top_indices = sims.argsort()[::-1][1:top_n+1]
    
    res = f"### Si te gusta **{target_title}**...\n\nTu modelo dice que sus guiones m√°s cercanos son:\n\n"
    for i in top_indices:
        res += f"- **{df.loc[i, 'title_display']}** (Similitud: {sims[i]*100:.1f}%)\n"
    
    return res

# Probar con varios cl√°sicos representativos de g√©neros opuestos
markdown_output = ""
markdown_output += get_closest("Inception") + "\n\n"
markdown_output += get_closest("Pulp Fiction") + "\n\n"
markdown_output += get_closest("Alien") + "\n\n"
markdown_output += get_closest("Notting Hill") + "\n\n"
markdown_output += get_closest("Star Wars") + "\n\n"

display(Markdown(markdown_output))
```

> Aqu√≠ es donde se lucen los embeddings. Si a **Inception** le salen pel√≠culas como *Interstellar* o thrillers de robos/sci-fi, o a **Alien** le salen *Prometheus* o pel√≠culas de claustrofobia espacial, demuestra que el NLP entiende contextualmente la premisa de la escena.

---

## Cap√≠tulo 6: Mega-Conclusiones (5 Observaciones Profundas)

Al abandonar la idea de "clusters est√°ticos" y mirar a este UMAP continuo, extraemos que:

### 1. El Mito del "Guion √önico" frente al Ecosistema Compartido
Al pintar las sagas (Harry Potter, Marvel, Star Wars), vemos que las secuelas caen sistem√°ticamente unas encima de las otras en subc√∫mulos apretados. Esto valida que las franquicias tienen **l√©xico cerrado y estructuras repetitivas**. El lenguaje en pantalla ("Repulsor beam", "Avada Kedavra", "The force") fuerza al algoritmo de embeddings a construir guettos lexicales.

### 2. El Espectro del Sentido y la Descriptividad
Enriquecer con m√©tricas de TextBlob demostr√≥ que la **Subjetividad** y el **Sentimiento** act√∫an como atractores. Hay una costa completa del continente oscuro (generalmente el lado rojo de nuestro scatter de Sentimiento) dominada por cintas de terror y thrillers policiacos, donde el tono general es severamente negativo. 

### 3. La "Gran Mancha de Acci√≥n" vs El "Archipi√©lago Dram√°tico"
El gr√°fico de contorno (Topograf√≠a) no miente. UMAP usualmente divide el dataset brutalmente en dos: 
- Un mega-continente de scripts con alta riqueza l√©xica y descripciones emocionales o de sala de estar (Dramas, Romances, Cine Negro).
- Una masa separada que carece de profundidad verbal pero rebosa de directivas cinem√°ticas de impacto (Acci√≥n, Aventura, Fantas√≠a).  

### 4. Directores-Firma o Pel√≠culas sin Patria
Hay vectores solitarios que no se alinean ni con la acci√≥n est√°ndar ni con el drama indie. Estas pel√≠culas suelen ser las de **Nolan, Tarantino, o los hermanos Coen**, que abusan de formatos narrativos que TextBlob y el embedding penalizan con aislamiento. Si dibuj√°ramos a Tarantino (alto en insultos, ritmo de di√°logo alt√≠simo y l√©xico diverso), se sit√∫a en islotes lejanos al script modelo de estudio.

### 5. Del Motor de Similitud al Frontend (Lo Que Sigue)
Todo este an√°lisis no es solo te√≥rico. Que Inception recomiende Interstellar puramente por el c√°lculo del coseno del texto de su guion, es el santo grial de la app Web en **React / Streamlit**. Significar√° que cuando un usuario ponga un prompt propio de ciencia ficci√≥n con robos corporativos en la web APP, nuestro buscador lo disparar√° directamente al sector cu√°ntico que acabamos de vislumbrar gr√°ficamente. La "Galaxia del Cine" cartograf√≠a el arte con precisi√≥n matem√°tica.